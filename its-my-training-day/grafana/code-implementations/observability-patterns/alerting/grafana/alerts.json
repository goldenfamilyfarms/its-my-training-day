{
  "_comment": "Grafana Alerting Configuration Examples",
  "_description": "This file contains Grafana alert rule examples demonstrating best practices for interview preparation",
  "_version": "Grafana 10.x+ Unified Alerting",
  
  "apiVersion": 1,
  "groups": [
    {
      "orgId": 1,
      "name": "SLO Alerts",
      "folder": "Service Level Objectives",
      "interval": "1m",
      "rules": [
        {
          "uid": "slo-availability-fast-burn",
          "title": "SLO Availability - Fast Burn",
          "condition": "C",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 3600,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "sum(rate(http_requests_total{status=~\"5..\"}[1h])) / sum(rate(http_requests_total[1h]))",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "B"
              }
            },
            {
              "refId": "C",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [0.0144],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  },
                  {
                    "evaluator": {
                      "params": [0.0144],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["B"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "C",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "2m",
          "annotations": {
            "summary": "High error rate burning SLO budget rapidly",
            "description": "Error rate exceeds 14.4x the SLO target. At this rate, monthly error budget will be exhausted in ~1 hour.",
            "runbook_url": "https://runbooks.example.com/slo-error-budget"
          },
          "labels": {
            "severity": "critical",
            "slo": "availability",
            "burn_rate": "fast"
          },
          "isPaused": false
        },
        {
          "uid": "slo-latency-p99",
          "title": "SLO Latency - P99 Exceeded",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [0.5],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "5m",
          "annotations": {
            "summary": "P99 latency exceeds SLO target of 500ms",
            "description": "Service {{ $labels.job }} p99 latency is above 500ms threshold."
          },
          "labels": {
            "severity": "warning",
            "slo": "latency"
          },
          "isPaused": false
        }
      ]
    },
    {
      "orgId": 1,
      "name": "Resource Alerts",
      "folder": "Infrastructure",
      "interval": "1m",
      "rules": [
        {
          "uid": "resource-cpu-high",
          "title": "High CPU Utilization",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [80],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "10m",
          "annotations": {
            "summary": "High CPU utilization detected",
            "description": "Instance {{ $labels.instance }} CPU utilization is above 80%."
          },
          "labels": {
            "severity": "warning",
            "resource": "cpu"
          },
          "isPaused": false
        },
        {
          "uid": "resource-memory-high",
          "title": "High Memory Usage",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [85],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "10m",
          "annotations": {
            "summary": "High memory usage detected",
            "description": "Instance {{ $labels.instance }} memory usage is above 85%."
          },
          "labels": {
            "severity": "warning",
            "resource": "memory"
          },
          "isPaused": false
        },
        {
          "uid": "resource-disk-low",
          "title": "Low Disk Space",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "(node_filesystem_avail_bytes{fstype!~\"tmpfs|overlay\"} / node_filesystem_size_bytes{fstype!~\"tmpfs|overlay\"}) * 100",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [20],
                      "type": "lt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "15m",
          "annotations": {
            "summary": "Low disk space warning",
            "description": "Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} has less than 20% free space."
          },
          "labels": {
            "severity": "warning",
            "resource": "disk"
          },
          "isPaused": false
        }
      ]
    },
    {
      "orgId": 1,
      "name": "Application Alerts",
      "folder": "Applications",
      "interval": "1m",
      "rules": [
        {
          "uid": "app-service-down",
          "title": "Service Down",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 60,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "up",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [1],
                      "type": "lt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "Alerting",
          "execErrState": "Alerting",
          "for": "1m",
          "annotations": {
            "summary": "Service is down",
            "description": "Service {{ $labels.job }} instance {{ $labels.instance }} is not responding.",
            "runbook_url": "https://runbooks.example.com/service-down"
          },
          "labels": {
            "severity": "critical"
          },
          "isPaused": false
        },
        {
          "uid": "app-high-error-rate",
          "title": "High Error Rate",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "sum by (job) (rate(http_requests_total{status=~\"5..\"}[5m])) / sum by (job) (rate(http_requests_total[5m])) * 100",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [5],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "5m",
          "annotations": {
            "summary": "High error rate detected",
            "description": "Service {{ $labels.job }} has error rate above 5%."
          },
          "labels": {
            "severity": "critical"
          },
          "isPaused": false
        },
        {
          "uid": "app-traffic-anomaly",
          "title": "Traffic Anomaly Detected",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "sum by (job) (rate(http_requests_total[5m])) / sum by (job) (rate(http_requests_total[1h]))",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [2],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "or"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  },
                  {
                    "evaluator": {
                      "params": [0.5],
                      "type": "lt"
                    },
                    "operator": {
                      "type": "or"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "10m",
          "annotations": {
            "summary": "Traffic anomaly detected",
            "description": "Service {{ $labels.job }} traffic is significantly different from the hourly average."
          },
          "labels": {
            "severity": "warning"
          },
          "isPaused": false
        }
      ]
    },
    {
      "orgId": 1,
      "name": "Kubernetes Alerts",
      "folder": "Kubernetes",
      "interval": "1m",
      "rules": [
        {
          "uid": "k8s-pod-crashloop",
          "title": "Pod CrashLooping",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 900,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [3],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "5m",
          "annotations": {
            "summary": "Pod is crash looping",
            "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted more than 3 times in 15 minutes.",
            "runbook_url": "https://runbooks.example.com/pod-crashloop"
          },
          "labels": {
            "severity": "critical"
          },
          "isPaused": false
        },
        {
          "uid": "k8s-deployment-replicas",
          "title": "Deployment Replicas Mismatch",
          "condition": "B",
          "data": [
            {
              "refId": "A",
              "queryType": "",
              "relativeTimeRange": {
                "from": 300,
                "to": 0
              },
              "datasourceUid": "prometheus",
              "model": {
                "expr": "kube_deployment_spec_replicas - kube_deployment_status_available_replicas",
                "intervalMs": 1000,
                "maxDataPoints": 43200,
                "refId": "A"
              }
            },
            {
              "refId": "B",
              "queryType": "",
              "relativeTimeRange": {
                "from": 0,
                "to": 0
              },
              "datasourceUid": "__expr__",
              "model": {
                "conditions": [
                  {
                    "evaluator": {
                      "params": [0],
                      "type": "gt"
                    },
                    "operator": {
                      "type": "and"
                    },
                    "query": {
                      "params": ["A"]
                    },
                    "reducer": {
                      "params": [],
                      "type": "last"
                    },
                    "type": "query"
                  }
                ],
                "refId": "B",
                "type": "classic_conditions"
              }
            }
          ],
          "noDataState": "NoData",
          "execErrState": "Error",
          "for": "10m",
          "annotations": {
            "summary": "Deployment has unavailable replicas",
            "description": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has fewer available replicas than desired."
          },
          "labels": {
            "severity": "warning"
          },
          "isPaused": false
        }
      ]
    }
  ],
  "contactPoints": [
    {
      "orgId": 1,
      "name": "slack-critical",
      "receivers": [
        {
          "uid": "slack-critical-receiver",
          "type": "slack",
          "settings": {
            "url": "${SLACK_WEBHOOK_URL}",
            "recipient": "#alerts-critical",
            "username": "Grafana Alerts",
            "icon_emoji": ":rotating_light:",
            "title": "{{ .CommonLabels.alertname }}",
            "text": "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}"
          },
          "disableResolveMessage": false
        }
      ]
    },
    {
      "orgId": 1,
      "name": "slack-warning",
      "receivers": [
        {
          "uid": "slack-warning-receiver",
          "type": "slack",
          "settings": {
            "url": "${SLACK_WEBHOOK_URL}",
            "recipient": "#alerts-warning",
            "username": "Grafana Alerts",
            "icon_emoji": ":warning:",
            "title": "{{ .CommonLabels.alertname }}",
            "text": "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}"
          },
          "disableResolveMessage": false
        }
      ]
    },
    {
      "orgId": 1,
      "name": "pagerduty-critical",
      "receivers": [
        {
          "uid": "pagerduty-receiver",
          "type": "pagerduty",
          "settings": {
            "integrationKey": "${PAGERDUTY_INTEGRATION_KEY}",
            "severity": "critical",
            "class": "{{ .CommonLabels.alertname }}",
            "component": "{{ .CommonLabels.job }}",
            "group": "{{ .CommonLabels.namespace }}"
          },
          "disableResolveMessage": false
        }
      ]
    },
    {
      "orgId": 1,
      "name": "email-team",
      "receivers": [
        {
          "uid": "email-receiver",
          "type": "email",
          "settings": {
            "addresses": "oncall@example.com",
            "singleEmail": true
          },
          "disableResolveMessage": false
        }
      ]
    }
  ],
  "policies": [
    {
      "orgId": 1,
      "receiver": "slack-warning",
      "group_by": ["alertname", "job"],
      "group_wait": "30s",
      "group_interval": "5m",
      "repeat_interval": "4h",
      "routes": [
        {
          "receiver": "pagerduty-critical",
          "matchers": ["severity=critical"],
          "group_wait": "10s",
          "group_interval": "1m",
          "repeat_interval": "1h",
          "continue": true
        },
        {
          "receiver": "slack-critical",
          "matchers": ["severity=critical"],
          "group_wait": "10s",
          "group_interval": "1m",
          "repeat_interval": "1h"
        },
        {
          "receiver": "slack-warning",
          "matchers": ["severity=warning"],
          "group_wait": "30s",
          "group_interval": "5m",
          "repeat_interval": "4h"
        },
        {
          "receiver": "email-team",
          "matchers": ["severity=~critical|warning"],
          "group_wait": "1m",
          "group_interval": "10m",
          "repeat_interval": "12h",
          "mute_time_intervals": ["outside-business-hours"]
        }
      ]
    }
  ],
  "muteTimeIntervals": [
    {
      "orgId": 1,
      "name": "outside-business-hours",
      "time_intervals": [
        {
          "times": [
            {
              "start_time": "18:00",
              "end_time": "09:00"
            }
          ],
          "weekdays": ["monday:friday"]
        },
        {
          "weekdays": ["saturday", "sunday"]
        }
      ]
    },
    {
      "orgId": 1,
      "name": "maintenance-window",
      "time_intervals": [
        {
          "times": [
            {
              "start_time": "02:00",
              "end_time": "04:00"
            }
          ],
          "weekdays": ["sunday"]
        }
      ]
    }
  ]
}
